

--------------------------------------------------------------------------------- READ ME -------------------------------------------------------------------------------------------


I built this automated machine learning pipeline to make binary classification tasks simple and straightforward. The pipeline takes care of everything from reading your data to training models and showing you the results. You simply provide a CSV file with your data, specify which column you want to predict, and the pipeline handles all the technical details automatically.
The pipeline starts by reading your CSV file and can automatically figure out how the data is formatted, whether it uses commas, semicolons, or tabs as separators. It then cleans up your data by fixing column names, removing unnecessary columns like IDs or customer names, and standardizing text values. If your target column contains words like "yes" and "no", it automatically converts them to numbers that machine learning models can understand.
Once the data is cleaned, I've implemented preprocessing steps that handle missing values and convert different types of data into formats that models can work with. For numerical columns like age or income, it fills in missing values and scales everything to a similar range. For categorical columns like gender or product type, I use different encoding strategies depending on how many unique values each column has. Columns with just a few categories get one-hot encoded, while columns with many categories get frequency encoded to keep things efficient.
I've set up the pipeline to train four different types of machine learning models on your data. It uses XGBoost and LightGBM, which are powerful gradient boosting models known for their accuracy. It also trains a Random Forest model and a neural network (MLP), and automatically tunes their settings to find the best performance. For datasets where one class is much more common than the other, I've included an option to enable SMOTE, which balances the classes by creating synthetic examples of the minority class.
After training, the pipeline evaluates each model and shows you how well they performed using multiple metrics including accuracy, precision, recall, F1-score, and ROC-AUC. I've built in detailed visualizations to help you understand the results, including confusion matrices that show what the model predicted versus what was actually true, ROC curves that display the tradeoff between true and false positives, and feature importance charts that reveal which variables had the biggest impact on predictions. The pipeline also generates comparison charts so you can easily see which model performed best.
Everything the pipeline creates gets saved to an output folder. This includes all the trained models, the preprocessing tools, and all the charts and metrics. This means you can use the same preprocessing and models on new data later, or share your results with others. I've made the entire process reproducible by using fixed random seeds, so you'll get the same results every time you run it with the same settings.
Requirements
You'll need to have Python installed with these libraries: pandas, numpy, scikit-learn, xgboost, lightgbm, imbalanced-learn, matplotlib, seaborn, and joblib.
Usage
To use this pipeline, open the script and find the NotebookArgs class near the bottom. Change the input field to point to your CSV file and set the target field to the name of the column you want to predict. You can also adjust other settings like the test size or whether to use SMOTE. Then simply run the script and it will create an output folder with all your results.
